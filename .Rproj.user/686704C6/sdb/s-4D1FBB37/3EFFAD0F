{
    "collab_server" : "",
    "contents" : "---\ntitle: \"sentiment analysis on trump tweets\"\noutput: html_document\n---\n\n```{r message=F, warning = F}\nlibrary(data.table) # CSV file I/O, e.g. the read_csv function\nlibrary(NLP)\nlibrary(openNLP)\nlibrary(plyr)\nlibrary(RSentiment)\nlibrary(gbm)\nlibrary(verification)\nlibrary(tm)\nlibrary(topicmodels)\n```\nNote these tweets can't be easily updated because bs4 is not installed on the Kaggle Kernel.\n\n```{r}\n#system(\"pip install bs4\")\n#system(\"python scrape.py\")\n```\n\nFirst let's have a look at these tweets\n```{r}\ntweets=fread(\"data.csv\")\nprint(\"here are the latest tweets\")\nhead(tweets)\nprint(\"here are the available variables\")\nnames(tweets)\n```\n\nBoilerplate code from openNLP R Documentation Examples related to function Maxent_POS_Tag_Annotator.\nIt will extract the synthaxic role of each word.\n```{r}\ns=as.String(tweets$Text[1])\nprint(s)\n## Need sentence and word token annotations.\nsent_token_annotator <- Maxent_Sent_Token_Annotator()\nword_token_annotator <- Maxent_Word_Token_Annotator()\na2 <- annotate(s, list(sent_token_annotator, word_token_annotator))\n\npos_tag_annotator <- Maxent_POS_Tag_Annotator()\npos_tag_annotator\na3 <- annotate(s, pos_tag_annotator, a2)\na3\n## Variant with POS tag probabilities as (additional) features.\nhead(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))\n\n## Determine the distribution of POS tags for word tokens.\na3w <- subset(a3, type == \"word\")\ntags <- sapply(a3w$features, `[[`, \"POS\")\ntags\ntable(tags)\n## Extract token/POS pairs (all of them): easy.\nsprintf(\"%s/%s\", s[a3w], tags)\n```\nNow let's see if we can identify a person in the tweets.\n```{r}\nlen = 0\ni=1\nsent_token_annotator <- Maxent_Sent_Token_Annotator()\nword_token_annotator <- Maxent_Word_Token_Annotator()\nentity_annotator <- Maxent_Entity_Annotator(language=\"en\",kind=\"person\",probs=TRUE)\nentity_annotator\nwhile (len ==0){\nprint(i)\ns=as.String(tweets$Text[i])\nprint(s)\n## Need sentence and word token annotations.\na2 <- annotate(s, list(sent_token_annotator, word_token_annotator))\n## Entity recognition for persons.\nannotate(s, entity_annotator, a2)\n## Directly:\nentity_annotator(s, a2)\npersons = s[entity_annotator(s,a2)]\nlen=length(persons)\ni=i+1\n}\n\n## Variant with sentence probabilities as features.\nannotate(s, Maxent_Entity_Annotator(probs = TRUE), a2)\n\n```\nIt's not 100% working, it did not annotate Hillary as a person but annotated Carlos Slim. Maybe it needs first name & last name.\n\nPrepare the text for sentiment analysis\n```{r}\nsome_txt=c(tweets$Text)\n# remove retweet entities\nsome_txt = gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\", some_txt)\n# remove at people\nsome_txt = gsub(\"@\\\\w+\", \"\", some_txt)\n# remove punctuation\nsome_txt = gsub(\"[[:punct:]]\", \" \", some_txt)\n# remove numbers\nsome_txt = gsub(\"[[:digit:]]\", \"\", some_txt)\n# remove html links\nsome_txt = gsub(\"http\\\\w+\", \"\", some_txt)\n# remove unnecessary spaces\nsome_txt = gsub(\"[ \\t]{2,}\", \" \", some_txt)\nsome_txt = gsub(\"^\\\\s+|\\\\s+$\", \" \", some_txt)\n\n# define \"tolower error handling\" function \ntry.error = function(x)\n{\n   # create missing value\n   y = NA\n   # tryCatch error\n   try_error = tryCatch(tolower(x), error=function(e) e)\n   # if not an error\n   if (!inherits(try_error, \"error\"))\n   y = tolower(x)\n   # result\n   return(y)\n}\n# lower case using try.error with sapply \nsome_txt = sapply(some_txt, try.error)\n\n# remove NAs in some_txt\nsome_txt = some_txt[!is.na(some_txt)]\nnames(some_txt) = NULL\n\n```\n\nLet's see how to make simple sentiment analysis with RSentiment\n```{r}\nlength(some_txt)\nsize=1000\ncalculate_total_presence_sentiment(c(\"This is good\",\"This is bad\"))\nsystem.time(sentiments_raw<-sapply(some_txt[1:size],function(x){\nreturn(c(calculate_total_presence_sentiment(x)[2,]))\n}))\nsentiments_DT=data.table(t(matrix(sentiments_raw,nrow=6)))\nnames(sentiments_DT)<-c(\"Sarcasm\",\"Neutral\",\"Negative\",\"Positive\",\"Very Negative\",\"Very Positive\")\nfor (nm in names(sentiments_DT)){\nsentiments_DT[[nm]]<-as.numeric(sentiments_DT[[nm]])\n}\nsave(file=\"sentiments.RData\",list=c(\"sentiments_DT\"))\n\ncolSums(sentiments_DT)\nstats=data.table()\nfor (nm in names(sentiments_DT)){\nnb=sum(sentiments_DT[[nm]])\nstats=rbind(stats,data.table(name=nm,fav_average=sum(sentiments_DT[[nm]]*tweets$Favorites[1:size])/nb,\nretweet_average=sum(sentiments_DT[[nm]]*tweets$Retweets[1:size])/nb,\nvolume=nb))\n}\nstats=stats[stats$volume>0]\nstats$name=factor(stats$name)\n```\nOf course the extremes are more popular.\n```{r}\nstats\nplot(x=stats$name,y=stats$fav_average)\n```\n\nLet's build a silly sentiment score to model sentiments\n\n```{r}\ntweets_sample=cbind(tweets[1:size],sentiments_DT)\nweights=c(\"sarcasm\"=-2,\"neutral\"=0,\"Negative\"=-1,\"positive\"=1,\"very_negative\"=-2,\"very_positive\"=2)\nweights\nscore=apply(sentiments_DT,1,function(x){\nsum(x*weights)\n})\nhead(score)\ntweets_sample$sentiment_score=score\ntweets_sample$tweet_length=nchar(tweets_sample$Text)\nnames(tweets_sample)\n\nmodelling_var=c(\"Favorites\",\"Retweets\",\"tweet_length\")\n\nfor (nm in modelling_var){\ntweets_sample[[nm]]=as.numeric(as.character(tweets_sample[[nm]]))\n}\n```\n\nLittle data preparation\n```{r}\ntweets_sample$Favorites=log(tweets_sample$Favorites) # this won't change anything to the GBM result but the dependency plots will be more interpretable\ntweets_sample$Retweets=log(tweets_sample$Retweets)\n```\n\nNow build a simple model\n```{r}\ntrain_sample=sample(1:size,round(0.7*size))\ngbm_params=c(shrinkage=0.002,nb_trees=500,depth=2)\nsummary(tweets_sample)\ngbm_model=gbm(sentiment_score ~Favorites+Retweets+tweet_length,\ndata=tweets_sample[train_sample],shrinkage=gbm_params[1],\nn.trees=gbm_params[2],interaction.depth=gbm_params[3],verbose=TRUE,\ntrain.fraction=0.7)\nsummary(gbm_model)\nplot(gbm_model,i.var=1)\nplot(gbm_model,i.var=2)\nplot(gbm_model,i.var=3)\nplot(gbm_model,i.var=c(1,2))\nplot(gbm_model,i.var=c(1,3))\nplot(gbm_model,i.var=c(2,3))\n\npred = predict(newdata=tweets_sample[-train_sample],object=gbm_model)\npred_sign = pred\npred_extreme = abs(pred)\n\nobs = tweets_sample[-train_sample]$sentiment_score\ngini_sign = (roc.area(obs=1*(obs>0),pred=pred_sign)$A -1/2)*2\ngini_extreme = (roc.area(obs=1*(abs(obs)>1),pred=pred_extreme)$A -1/2)*2\ngini_sign\ngini_extreme\n```\n\nLet's improve our model with new features.\nFor that, we are going to make topics modelling.\n\n```{r}\nstrsplit_space_tokenizer=function(x)\n  unlist(strsplit(as.character(x), \"[[:space:]]+\"))\nctrl=list(tokenize = strsplit_space_tokenizer,\n             removePunctuation = list(preserve_intra_word_dashes = TRUE),\n             stemming = TRUE,\n             wordLengths = c(4, Inf),\n             language=\"en\")\nTF_CTRL=termFreq(tweets_sample$Text, control = ctrl)\nfindMostFreqTerms(TF_CTRL,n = 30)\ntopics_number=10\ntopics = LDA(x = TF_CTRL,k = topics_number,method=\"Gibbs\")\nsummary(topics)\n\nlibrary(tidytext)\nlibrary(ggplot2)\ngetTopicToClaimDistribution = function(ldaModel){\n  ldaGamma <- tidytext::tidy(ldaModel, matrix = \"gamma\")\n  ldaGamma\n}\n\ngetTopicToClaimDistribution(topics)\n#lda_inf = posterior(topics,tweets_sample$Text)\n```\n\nAny results you write to the current directory are saved as output.\n",
    "created" : 1494330041992.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "503447331",
    "id" : "3EFFAD0F",
    "lastKnownWriteTime" : 1494330902,
    "last_content_update" : 1494330902,
    "path" : "~/Documents/trump tweets/sentiment analysis.Rmd",
    "project_path" : "sentiment analysis.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}